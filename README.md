# AI-Powered Customer Retention Prediction System
---
ğŸ§  Goal

The goal of this project is to develop a Machine Learningâ€“based Churn Prediction System that predicts whether a telecom customer will leave (Churn) or stay (Retain) based on their service usage, billing details, and demographic patterns.
This helps telecom companies take proactive actions like customer retention campaigns, discount offers, and service improvements.
------
ğŸ“‚ Dataset Overview
Source: Telco_Data_With_Tax_Gateway_Updated.csv
Rows: 7043
Columns: 21 features + Target (Churn)
Target Variable: Churn (Yes = Customer left, No = Customer stayed)

## ğŸ”‘ Key Features

| **Feature** | **Description** |
|--------------|-----------------|
| Gender | Male / Female |
| Partner / Dependents | Customer relationship info |
| InternetService | DSL / Fiber optic / None |
| PaymentMethod | Electronic check, Credit card, etc. |
| Contract | Month-to-month / One year / Two year |
| Tenure, MonthlyCharges, TotalCharges | Numeric variables |
| Churn | Target variable (Yes/No) |

-----
## ğŸ“Š End-to-End Project Workflow â€” AI-Powered Customer Retention Prediction System

---

### 1ï¸âƒ£ Data Uncleaned â†’ Data Cleaning Process

**ğŸ¯ Goal:**  
Prepare raw telecom dataset for machine learning.

**ğŸ› ï¸ Actions Performed:**
- Removed irrelevant column: `customerID`
- Converted invalid strings in `TotalCharges` to numeric
- Handled missing and blank values in key features
- Checked for datatype mismatches, duplicates, and whitespace issues

**âœ… Result:**  
Clean and structured dataset ready for preprocessing.

---
### 2ï¸âƒ£ Feature Engineering / Missing Value Imputation

**ğŸ¯ Goal:**  
Replace missing values effectively while preserving statistical integrity.

**âš™ï¸ Methods Tried:**

| **Method** | **Description** | **Observation** |
|-------------|-----------------|-----------------|
| Forward Fill | Replaced missing values using previous non-null entries. | Worked for time-series style data but not random missingness. |
| Backward Fill | Used next valid value to fill NaNs. | Similar limitations as forward fill. |
| Simple Imputer | Replaced with mean/median. | Too basic, lost variance information. |
| Iterative Imputer | Modeled missing values using other variables iteratively. | Gave stable results but slightly slower. |
| KNN Imputer | Used K-nearest neighbors to estimate missing values. | âœ… Most accurate for continuous & correlated features. |

**âœ… Finalized:**  
`KNNImputer()`

**ğŸ“ Reason:**  
It leverages feature similarity to fill gaps â€” ideal for this structured telecom dataset with correlated numeric features like `MonthlyCharges`, `TotalCharges`, and `tenure`.

---
### 3ï¸âƒ£ Variable Transformation

**ğŸ¯ Goal:**  
Transform non-normal features to approximate a Gaussian distribution and improve model stability.

**âš™ï¸ Methods Tried:**

| **Transformation** | **Description** | **Observation** |
|---------------------|-----------------|-----------------|
| Log Transform | For right-skewed variables | Not suitable for zero/negative values |
| Arcsin | Works for proportion data | Not applicable here |
| Box-Cox | Strong but only positive data | Limited usability |
| Yeo-Johnson | Handles negative/zero | Good results, slight skew |
| Quantile Transformer | Maps data to uniform/normal distribution | âœ… Excellent normalization across all numeric features |

**âœ… Finalized:**  
`QuantileTransformer(output_distribution='normal')`

**ğŸ“ Reason:**  
Provides smooth Gaussian-like data distribution, preserving outlier structure while improving model convergence.

---
## 4ï¸âƒ£ Handling Outliers

**ğŸ¯ Goal:**  
Reduce outlier influence to stabilize model training.

| **Method** | **Description** | **Observation** |
|-------------|-----------------|-----------------|
| Power Transformer | Normalized variance but distorted relationships |  |
| Quantile Transform | Reduced extreme values but overly smooth |  |
| Winsorizer | Caps extreme values using IQR range | âœ… Balanced trimming and preserved shape |

**âœ… Finalized:**  
Winsorizer (IQR-based)

**ğŸ“ Reason:**  
Winsorizing effectively capped extreme billing outliers without data loss â€” especially in `MonthlyCharges` and `TotalCharges`.

---

## 5ï¸âƒ£ Feature Selection

**ğŸ¯ Goal:**  
Select the most relevant features and remove low-variance or redundant ones.

**ğŸ§® Filter Methods Used:**
- **Constant Method:** Removed features with a single unique value.  
- **Quasi-Constant Method:** Removed features with very low variance (<1%).

**âœ… Result:**  
Improved feature set with only meaningful variation retained.

---

## 6ï¸âƒ£ Categorical â†’ Numerical Encoding

**ğŸ¯ Goal:**  
Convert categorical data into machine-understandable numerical format.

| **Encoding Type** | **Columns** | **Reason** |
|--------------------|-------------|-------------|
| Ordinal Encoding | `Contract` (Month-to-month â†’ 0, One year â†’ 1, Two year â†’ 2) | Natural order hierarchy |
| Label Encoding | `Churn (Yes=1, No=0)` | Binary target variable |
| One-Hot Encoding | `gender`, `InternetService`, `PaymentMethod`, `OnlineSecurity`, `OnlineBackup`, `DeviceProtection`, `TechSupport`, `StreamingTV`, `StreamingMovies`, `PaperlessBilling`, `MultipleLines` | Non-ordinal multi-class categorical variables |

**âœ… Result:**  
Transformed categorical columns into a structured numeric feature space for modeling.

---

## 7ï¸âƒ£ Hypothesis Testing

**ğŸ¯ Goal:**  
Statistically verify feature significance with respect to churn.

| **Method** | **Suitable For** | **Purpose** |
|-------------|------------------|--------------|
| Chi-Square Test | Categorical vs Target | Test dependence between churn and categorical features |
| ANOVA | Continuous vs Target | Compare means across churn groups |
| Correlation Matrix | Numeric-Numeric | Measure linear relationships |

**âœ… Finalized:**  
`Chi-Square Test`

**ğŸ“ Reason:**  
Ideal for categorical telecom data; effectively identified high-impact features like `Contract`, `InternetService`, and `TechSupport`.

---

## 8ï¸âƒ£ Merging Data

**ğŸ¯ Goal:**  
Combine numeric and encoded categorical data into a single training dataset.

**âœ… Action:**  
`pd.concat([train_num, train_cat], axis=1)` after encoding and scaling.

**âœ… Result:**  
Unified dataset for model training with consistent indexing.

---

## 9ï¸âƒ£ Balancing Data (SMOTE)

**ğŸ¯ Goal:**  
Handle target class imbalance since â€œChurn = Yesâ€ cases were underrepresented.

**âœ… Used:**  
`SMOTE (Synthetic Minority Oversampling Technique)`

**ğŸ“ Reason:**  
Creates synthetic samples for the minority class, improving recall and reducing bias.

**âœ… Result:**  
Balanced target distribution â€” improved model fairness and generalization.

---

## ğŸ”Ÿ Train All Machine Learning Models

**ğŸ§  Models Trained & Compared:**

| **Model** | **Type** | **Performance** |
|------------|-----------|----------------|
| Logistic Regression | Linear Classifier | â­ Excellent interpretability |
| Decision Tree | Tree-based | Overfit slightly |
| Random Forest | Ensemble | Stable but slower |
| K-Nearest Neighbors | Distance-based | Moderate accuracy |
| NaÃ¯ve Bayes | Probabilistic | Poor fit for mixed data |

---

## 1ï¸âƒ£1ï¸âƒ£ Model Selection using ROC-AUC

**ğŸ“ˆ Metric Used:**  
*AUC-ROC Curve (Area Under the Receiver Operating Characteristic)*

| **Model** | **AUC-ROC** | **Result** |
|------------|-------------|-------------|
| Logistic Regression | 0.77 âœ… | Best |
| Random Forest | 0.72 | Good |
| Decision Tree | 0.67 | Acceptable |
| KNN | 0.71 | Lower accuracy |

**âœ… Finalized Model:**  
`Logistic Regression`

**ğŸ“ Reason:**  
Highest AUC, interpretable coefficients, consistent probability outputs, minimal overfitting.

---

## 1ï¸âƒ£2ï¸âƒ£ Train on Best Model

**âš™ï¸ Steps:**
- Re-trained Logistic Regression on the **full balanced dataset**
- Used scaled numeric features
- Applied optimized hyperparameters: `C=1.0`, `solver='liblinear'`
- Final feature set after Chi-square filtering

**âœ… Output:**  
Saved final performance metrics and model artifacts.

---

## 1ï¸âƒ£3ï¸âƒ£ Save Model Artifacts

**ğŸ§¾ Pickled for Deployment:**

| **File** | **Description** |
|-----------|-----------------|
| `churn_prediction.pkl` | Trained Logistic Regression model |
| `standard_scalar.pkl` | StandardScaler used for numeric features |
| `model_features.pkl` | Feature column order used in model |

**âœ… Benefit:**  
Ensures consistent real-time predictions in the Flask app.
---

## 1ï¸âƒ£5ï¸âƒ£ Prediction Output

**ğŸ¯ Goal:**  
Predict if the customer will churn or stay.

**ğŸ“Š Example Output:**
ğŸŸ¥ Customer will CHURN
Probability: 78.3%

âœ… Customer will STAY
Probability: 21.7%

**âœ… Business Use:**  
Telecom teams can focus on **high-risk customers** with churn probability > 70%,  
enhancing customer retention and reducing revenue loss.

---
## ğŸ§° Tools & Libraries

**ğŸ§® Core Languages & Frameworks**
- Python 3.9+
- Flask (for web app deployment)
- Render (for cloud hosting)

**ğŸ“¦ Libraries Used**
- NumPy  
- Pandas  
- Matplotlib  
- Seaborn  
- scikit-learn  
- feature-engine  
- imblearn  
- xgboost  

---

## ğŸ“Š Key Insights

- Customers with **month-to-month contracts** churn more frequently.  
- **Fiber optic** users have higher churn rates.  
- Customers **without dependents or partners** are more likely to leave.  
- **Electronic check** payment method strongly correlates with higher churn.  
- **Tenure** and **contract length** are strong predictors of customer retention.  

---
---

## ğŸ‘¨â€ğŸ’» Developer 
-----
   Nikhil Kumar

**ğŸ’¼ Machine Learning Engineer | AI & ML Enthusiast | Data Science Enthusiast**

---

### ğŸ§¾ Background

Hi! Iâ€™m **V. Nikhil Kumar**, a passionate **Machine Learning Engineer** with a strong interest in data-driven solutions.  
I specialize in building **predictive models**, **automating ML pipelines**, and developing **end-to-end machine learning web applications** that solve real-world problems.

---

### ğŸ’ª Skills

| **Category** | **Technologies / Tools** |
|---------------|---------------------------|
| Programming | Python |
| Machine Learning | scikit-learn, XGBoost, feature-engine, imblearn |
| Deep Learning | TensorFlow / Keras (basics) |
| Web Development | Flask |
| Data Visualization | Matplotlib, Seaborn |
| Databases | SQL |
| Version Control | Git & GitHub |

---

### ğŸ’¼ Previous Works

| **Project** | **Description** | **Live Link** |
|--------------|-----------------|---------------|
| ğŸ“Š **Credit Card Customer Analysis** | Data-driven insights into customer credit usage patterns. | ğŸ”— *Coming Soon* |
| ğŸ’° **Salary and Profit Predictor** | ML regression project for profit prediction. | ğŸŒ [**View Project â†—**](https://simple-and-multiple-regression-project.onrender.com/) |

---

### ğŸ“ Contact

- **LinkedIn:** [linkedin.com/in/nikhilkumar91](https://linkedin.com/in/nikhilkumar91)  
- **Email:** [nikhilkumarchary30@gmail.com](mailto:nikhilkumarchary30@gmail.com)  
- **GitHub:** [github.com/Nikhilkumar91](https://github.com/Nikhilkumar91)  
- **Mobile:** +91 9133164879  

---

> ğŸŒŸ *Built with passion for AI, Data, and Real-world Impact.*

---

